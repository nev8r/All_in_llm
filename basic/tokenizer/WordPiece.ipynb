{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6caa5de6",
   "metadata": {},
   "source": [
    "## WordPiece\n",
    "与BPE算法类似，但是有些许不同\n",
    "1. 合并规则不同，BPE每次合并选取当前pairs中频率最大的pair进行合并，而WordPiece按如下公式选取每次合并的pair\n",
    "$$ score = \\frac{freq\\_of\\_pair}{(freq\\_of\\_first\\_element x freq\\_of\\_second\\_element)}$$\n",
    "2. 表示方式略有不同，如 'hugging' -> (\"hu\", \"##gging\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9b7376",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1938c9",
   "metadata": {},
   "source": [
    "### 一、准备语料，计算word_freqs,初始化vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "8d575e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32ed29f",
   "metadata": {},
   "source": [
    "首先使用pre_tokenize处理字符串，转为word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "7f5b4e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hopefully',\n",
       " ',',\n",
       " 'you',\n",
       " 'will',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'how',\n",
       " 'they',\n",
       " 'are',\n",
       " 'trained',\n",
       " 'and',\n",
       " 'generate',\n",
       " 'tokens',\n",
       " '.']"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "from typing import List\n",
    "\n",
    "def pre_tokenize(text: str, do_lower_case: bool = False) -> List[str]:\n",
    "    if do_lower_case:\n",
    "        text = text.lower()\n",
    "    # 去控制字符\n",
    "    text = re.sub(r\"[\\u0000-\\u001F\\u007F]\", \"\", text)\n",
    "    # 给 CJK 字符加空格\n",
    "    text = re.sub(r\"([\\p{Han}\\p{Hiragana}\\p{Katakana}\\p{Hangul}])\", r\" \\1 \", text)\n",
    "    # 给标点加空格（Unicode Punctuation）\n",
    "    text = re.sub(r\"([\\p{P}])\", r\" \\1 \", text)\n",
    "    # 按空格拆分\n",
    "    tokens = re.split(r\"\\s+\", text.strip())\n",
    "    return [t for t in tokens if t]\n",
    "\n",
    "pre_tokenize(corpus[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b04de",
   "metadata": {},
   "source": [
    "计算word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "ba5f13c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('T', '##h', '##i', '##s'): 3, ('i', '##s'): 2, ('t', '##h', '##e'): 1, ('H', '##u', '##g', '##g', '##i', '##n', '##g'): 1, ('F', '##a', '##c', '##e'): 1, ('C', '##o', '##u', '##r', '##s', '##e'): 1, ('.',): 4, ('c', '##h', '##a', '##p', '##t', '##e', '##r'): 1, ('a', '##b', '##o', '##u', '##t'): 1, ('t', '##o', '##k', '##e', '##n', '##i', '##z', '##a', '##t', '##i', '##o', '##n'): 1, ('s', '##e', '##c', '##t', '##i', '##o', '##n'): 1, ('s', '##h', '##o', '##w', '##s'): 1, ('s', '##e', '##v', '##e', '##r', '##a', '##l'): 1, ('t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'): 1, ('a', '##l', '##g', '##o', '##r', '##i', '##t', '##h', '##m', '##s'): 1, ('H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'): 1, (',',): 1, ('y', '##o', '##u'): 1, ('w', '##i', '##l', '##l'): 1, ('b', '##e'): 1, ('a', '##b', '##l', '##e'): 1, ('t', '##o'): 1, ('u', '##n', '##d', '##e', '##r', '##s', '##t', '##a', '##n', '##d'): 1, ('h', '##o', '##w'): 1, ('t', '##h', '##e', '##y'): 1, ('a', '##r', '##e'): 1, ('t', '##r', '##a', '##i', '##n', '##e', '##d'): 1, ('a', '##n', '##d'): 1, ('g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'): 1, ('t', '##o', '##k', '##e', '##n', '##s'): 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "def word_split(word):\n",
    "    return [word[0]] + [f\"##{ch}\" for ch in word[1:]]\n",
    "\n",
    "def get_word_freqs(corpus):\n",
    "    word_freqs = defaultdict(int)\n",
    "    for text in corpus:\n",
    "        words = pre_tokenize(text)\n",
    "        for word in words:\n",
    "            word_freqs[tuple(word_split(word))] += 1\n",
    "    return word_freqs\n",
    "\n",
    "print(get_word_freqs(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3c0e5",
   "metadata": {},
   "source": [
    " 初始化vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "1874502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "def get_vocab(corpus : str,special_tokens:List[str] = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]):\n",
    "    voacb = []\n",
    "    word_freqs = get_word_freqs(corpus)\n",
    "    for word in word_freqs.keys():\n",
    "        if word[0] not in voacb:\n",
    "            voacb.append(word[0])\n",
    "        for ch in word[1:]:\n",
    "            if ch not in voacb:\n",
    "                voacb.append(ch)\n",
    "    return special_tokens + sorted(voacb)\n",
    "print(get_vocab(corpus))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93356ba0",
   "metadata": {},
   "source": [
    "### 二、 计算分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "fb879fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('T', '##h'): 0.125,\n",
       " ('##h', '##i'): 0.03409090909090909,\n",
       " ('##i', '##s'): 0.02727272727272727,\n",
       " ('i', '##s'): 0.1,\n",
       " ('t', '##h'): 0.03571428571428571,\n",
       " ('##h', '##e'): 0.011904761904761904,\n",
       " ('H', '##u'): 0.1,\n",
       " ('##u', '##g'): 0.05,\n",
       " ('##g', '##g'): 0.0625,\n",
       " ('##g', '##i'): 0.022727272727272728,\n",
       " ('##i', '##n'): 0.01652892561983471,\n",
       " ('##n', '##g'): 0.022727272727272728,\n",
       " ('F', '##a'): 0.14285714285714285,\n",
       " ('##a', '##c'): 0.07142857142857142,\n",
       " ('##c', '##e'): 0.023809523809523808,\n",
       " ('C', '##o'): 0.07692307692307693,\n",
       " ('##o', '##u'): 0.046153846153846156,\n",
       " ('##u', '##r'): 0.022222222222222223,\n",
       " ('##r', '##s'): 0.022222222222222223,\n",
       " ('##s', '##e'): 0.004761904761904762,\n",
       " ('c', '##h'): 0.125,\n",
       " ('##h', '##a'): 0.017857142857142856,\n",
       " ('##a', '##p'): 0.07142857142857142,\n",
       " ('##p', '##t'): 0.07142857142857142,\n",
       " ('##t', '##e'): 0.013605442176870748,\n",
       " ('##e', '##r'): 0.026455026455026454,\n",
       " ('a', '##b'): 0.2,\n",
       " ('##b', '##o'): 0.038461538461538464,\n",
       " ('##u', '##t'): 0.02857142857142857,\n",
       " ('t', '##o'): 0.04395604395604396,\n",
       " ('##o', '##k'): 0.07692307692307693,\n",
       " ('##k', '##e'): 0.047619047619047616,\n",
       " ('##e', '##n'): 0.017316017316017316,\n",
       " ('##n', '##i'): 0.01652892561983471,\n",
       " ('##i', '##z'): 0.09090909090909091,\n",
       " ('##z', '##a'): 0.07142857142857142,\n",
       " ('##a', '##t'): 0.04081632653061224,\n",
       " ('##t', '##i'): 0.025974025974025976,\n",
       " ('##i', '##o'): 0.013986013986013986,\n",
       " ('##o', '##n'): 0.013986013986013986,\n",
       " ('s', '##e'): 0.031746031746031744,\n",
       " ('##e', '##c'): 0.023809523809523808,\n",
       " ('##c', '##t'): 0.07142857142857142,\n",
       " ('s', '##h'): 0.041666666666666664,\n",
       " ('##h', '##o'): 0.009615384615384616,\n",
       " ('##o', '##w'): 0.07692307692307693,\n",
       " ('##w', '##s'): 0.05,\n",
       " ('##e', '##v'): 0.047619047619047616,\n",
       " ('##v', '##e'): 0.047619047619047616,\n",
       " ('##r', '##a'): 0.047619047619047616,\n",
       " ('##a', '##l'): 0.02040816326530612,\n",
       " ('##z', '##e'): 0.023809523809523808,\n",
       " ('a', '##l'): 0.02857142857142857,\n",
       " ('##l', '##g'): 0.03571428571428571,\n",
       " ('##g', '##o'): 0.019230769230769232,\n",
       " ('##o', '##r'): 0.008547008547008548,\n",
       " ('##r', '##i'): 0.010101010101010102,\n",
       " ('##i', '##t'): 0.012987012987012988,\n",
       " ('##t', '##h'): 0.017857142857142856,\n",
       " ('##h', '##m'): 0.125,\n",
       " ('##m', '##s'): 0.1,\n",
       " ('H', '##o'): 0.038461538461538464,\n",
       " ('##o', '##p'): 0.038461538461538464,\n",
       " ('##p', '##e'): 0.023809523809523808,\n",
       " ('##e', '##f'): 0.047619047619047616,\n",
       " ('##f', '##u'): 0.2,\n",
       " ('##u', '##l'): 0.02857142857142857,\n",
       " ('##l', '##l'): 0.04081632653061224,\n",
       " ('##l', '##y'): 0.07142857142857142,\n",
       " ('y', '##o'): 0.07692307692307693,\n",
       " ('w', '##i'): 0.09090909090909091,\n",
       " ('##i', '##l'): 0.012987012987012988,\n",
       " ('b', '##e'): 0.047619047619047616,\n",
       " ('##b', '##l'): 0.07142857142857142,\n",
       " ('##l', '##e'): 0.006802721088435374,\n",
       " ('u', '##n'): 0.09090909090909091,\n",
       " ('##n', '##d'): 0.06818181818181818,\n",
       " ('##d', '##e'): 0.011904761904761904,\n",
       " ('##s', '##t'): 0.014285714285714285,\n",
       " ('##t', '##a'): 0.02040816326530612,\n",
       " ('##a', '##n'): 0.012987012987012988,\n",
       " ('h', '##o'): 0.07692307692307693,\n",
       " ('##e', '##y'): 0.023809523809523808,\n",
       " ('a', '##r'): 0.022222222222222223,\n",
       " ('##r', '##e'): 0.005291005291005291,\n",
       " ('t', '##r'): 0.015873015873015872,\n",
       " ('##a', '##i'): 0.012987012987012988,\n",
       " ('##n', '##e'): 0.008658008658008658,\n",
       " ('##e', '##d'): 0.011904761904761904,\n",
       " ('a', '##n'): 0.01818181818181818,\n",
       " ('g', '##e'): 0.047619047619047616,\n",
       " ('##n', '##s'): 0.00909090909090909}"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def compute_pair_scores(word_freqs):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word,freq in word_freqs.items():\n",
    "        if len(word) == 1:\n",
    "            letter_freqs[word[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(word) - 1):\n",
    "            pair = (word[i],word[i + 1])\n",
    "            letter_freqs[word[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[word[-1]] += freq\n",
    "    score = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]]) \n",
    "        for pair,freq in pair_freqs.items() \n",
    "    }\n",
    "    return score\n",
    "compute_pair_scores(get_word_freqs(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cec6a27",
   "metadata": {},
   "source": [
    "### 三、合并score最高的pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "3e1f1c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('a', '##b'), 0.2)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bestpair(pair_scores:dict):\n",
    "    best_pair = \"\"\n",
    "    max_score = None\n",
    "    for pair, score in pair_scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    return best_pair,max_score\n",
    "get_bestpair(compute_pair_scores(get_word_freqs(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "a8d76c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('T', '##h', '##i', '##s'): 3,\n",
       "             ('i', '##s'): 2,\n",
       "             ('t', '##h', '##e'): 1,\n",
       "             ('H', '##u', '##g', '##g', '##i', '##n', '##g'): 1,\n",
       "             ('F', '##a', '##c', '##e'): 1,\n",
       "             ('C', '##o', '##u', '##r', '##s', '##e'): 1,\n",
       "             ('.',): 4,\n",
       "             ('c', '##h', '##a', '##p', '##t', '##e', '##r'): 1,\n",
       "             ('ab', '##o', '##u', '##t'): 1,\n",
       "             ('t',\n",
       "              '##o',\n",
       "              '##k',\n",
       "              '##e',\n",
       "              '##n',\n",
       "              '##i',\n",
       "              '##z',\n",
       "              '##a',\n",
       "              '##t',\n",
       "              '##i',\n",
       "              '##o',\n",
       "              '##n'): 1,\n",
       "             ('s', '##e', '##c', '##t', '##i', '##o', '##n'): 1,\n",
       "             ('s', '##h', '##o', '##w', '##s'): 1,\n",
       "             ('s', '##e', '##v', '##e', '##r', '##a', '##l'): 1,\n",
       "             ('t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'): 1,\n",
       "             ('a',\n",
       "              '##l',\n",
       "              '##g',\n",
       "              '##o',\n",
       "              '##r',\n",
       "              '##i',\n",
       "              '##t',\n",
       "              '##h',\n",
       "              '##m',\n",
       "              '##s'): 1,\n",
       "             ('H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'): 1,\n",
       "             (',',): 1,\n",
       "             ('y', '##o', '##u'): 1,\n",
       "             ('w', '##i', '##l', '##l'): 1,\n",
       "             ('b', '##e'): 1,\n",
       "             ('ab', '##l', '##e'): 1,\n",
       "             ('t', '##o'): 1,\n",
       "             ('u',\n",
       "              '##n',\n",
       "              '##d',\n",
       "              '##e',\n",
       "              '##r',\n",
       "              '##s',\n",
       "              '##t',\n",
       "              '##a',\n",
       "              '##n',\n",
       "              '##d'): 1,\n",
       "             ('h', '##o', '##w'): 1,\n",
       "             ('t', '##h', '##e', '##y'): 1,\n",
       "             ('a', '##r', '##e'): 1,\n",
       "             ('t', '##r', '##a', '##i', '##n', '##e', '##d'): 1,\n",
       "             ('a', '##n', '##d'): 1,\n",
       "             ('g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'): 1,\n",
       "             ('t', '##o', '##k', '##e', '##n', '##s'): 1})"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_pair(a,b,word_freqs):\n",
    "    new_word_freqs = defaultdict(int)\n",
    "    for word,freq in word_freqs.items():\n",
    "        if len(word) == 1:\n",
    "            new_word_freqs[word] = freq\n",
    "            continue\n",
    "        i = 0\n",
    "        new_word = []\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and word[i] == a and word[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                new_word.append(merge)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        new_word_freqs[tuple(new_word)] = freq\n",
    "    return new_word_freqs\n",
    "merge_pair('a',\"##b\",get_word_freqs(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a660ec",
   "metadata": {},
   "source": [
    "### 四、train,定义vocab_size，重复2,3步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "fab7b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vocab_size,corpus):\n",
    "    merges = []\n",
    "    vocab = get_vocab(corpus)\n",
    "    word_freqs = get_word_freqs(corpus)\n",
    "    while len(vocab) < vocab_size:\n",
    "        pair_scores = compute_pair_scores(word_freqs)\n",
    "        best_pair,_ = get_bestpair(pair_scores)\n",
    "        a,b = best_pair[0],best_pair[1]\n",
    "        vocab.append(a + b[2:] if b.startswith(\"##\") else a + b)\n",
    "        merges.append(best_pair)\n",
    "        word_freqs = merge_pair(a,b,word_freqs)\n",
    "    return vocab,merges\n",
    "vocab,merges = train(70,corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d6314",
   "metadata": {},
   "source": [
    "### 五、使用训练好的merge,vocab进行encode和decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "f9e353e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Th', '##i', '##s', 'is', 'th', '##e', 'Hugg', '##i', '##n', '##g', 'Fac', '##e', 'c', '##o', '##u', '##r', '##s', '##e', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens\n",
    "def tokenize(text):\n",
    "    tokens = pre_tokenize(text)\n",
    "    return [tk for token in tokens for tk in encode_word(token) ]\n",
    "print(tokenize(\"This is the Hugging Face course!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "All_in_llm (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
